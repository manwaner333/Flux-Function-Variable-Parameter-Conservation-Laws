version_1 和 version_2 是两种完全相反的研究思路
1. 以 f(u) = 1/2u(3 - u^2) + beta/12 u^2 (3/4 - 2u + 3/2 u^2 - 1/4 u^4)  为例子，
version_1 是想同时学习变量u 和 beta, 把两者一起放在神经网络中， 学习出他们在方程中各自扮演的角色。
由于u 和 beta本身的意义是不一样的， 因为尝试设计了几种不同的expr.
(1) expr.py 里面， 变量u有一个子神经网络对应去学习，得出可能的组合形式(u, u^2, u^3, u^4......) 参数beta有一个神经网络对应去学习, 得出可能的组合形式
(p, p^2, ......) , 然后两者结合成为 （u, u^2, u^3, u^4， pu, pu^2, pu^3, pu^4, p^2u, p^2u^2, p^2u^3, p^2u^4, .....）

(2) expr_test_1.py 里面，用一个子神经网络学习和beta无关的项， 即目标去学习1/2u(3 - u^2)； 用一个子神经网络去学习和beta有关的项， 即学习
1/12 u^2 (3/4 - 2u + 3/2 u^2 - 1/4 u^4)， 然后直接乘以 beta.

(3) expr_test_1.py 里面，非常粗暴的想包含变量的项固定住就是(u, u^2, u^3, u^4......), 然后去直接乘以bata， 最后合并得出 （u, u^2, u^3,
u^4， beta*u, beta*u^2, beta*u^3, beta*u^4............）

(4) version_1 还生产用作对照实验graph net 的数据

这几种方式尝试下来， 都没有取得好的效果

2. version_2的思路是比较巧妙的， 既然我已经能够求得在不同bata下面的值，那么我可以假设方程f(u)=a1*u + a2*u*2 +  a3*u*3....
 那么我再借助另外一个神经网络或者回归的思路， 利用f(u)或者f(u)的导数， 直接去拟合这些参数就可以了。 这样每一个位置上的样本可以看成是一个单独的样本， 打破了位置的局限。
 此处需要注意， 如果利用f(u)需要知道f(u=0)处的值； 如果利用f(u)的导数， 可以不用知道原始值， 但是对应如果f(u)是除法的形式， 将是非常困难的。
 这个地方， numerical scheme 的版本进行了更新，利用了局部信息值， 其实增加了学习的难度。

 src_beta_10 和 src_beta_10_old 修正了f_half中取局部值得方法， 使得更加简便明了。

 3. version_3 相比较与version_2, 求解各个子问题时候利用的神经网络是除法形式的神经网络
 version_3中 * 相比于*_1来说， 修正了loss的形式， 同时调整了numerical scheme的形式， 使得u_fixed_np \in [0, 1]
 du = 1.0/500
 u_fixed_0 = 0.0
 u_fixed_np = np.zeros((1, 501), dtype=float)
 u_fixed_np[:1, 0] = u_fixed_0
 for i in range(1, 501):
     u_fixed_np[:1, i] = u_fixed_0 + i * du